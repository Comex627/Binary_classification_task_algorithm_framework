{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps,\n",
      "E:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "E:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "E:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "E:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1074: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,\n",
      "E:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1306: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=1, eps=np.finfo(np.float).eps,\n",
      "E:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1442: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "E:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  precompute=False, eps=np.finfo(np.float).eps,\n",
      "E:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:318: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, random_state=None,\n",
      "E:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\randomized_l1.py:575: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=4 * np.finfo(np.float).eps, n_jobs=1,\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb  \n",
    "from sklearn.linear_model import LogisticRegression  # 导入逻辑回归\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_recall_curve, roc_curve\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler  # 新增标准化器\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import datetime\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import metrics\n",
    "\n",
    "# 可视化相关库\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# 设置中文字体，解决乱码问题\n",
    "plt.rcParams[\"font.family\"] = [\"SimHei\", \"WenQuanYi Micro Hei\", \"Heiti TC\"]\n",
    "sns.set(font='SimHei', font_scale=0.8)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin ratio clcik...\n",
      "The end\n"
     ]
    }
   ],
   "source": [
    "# 数据读取\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "label = pd.read_csv('../data/train_label.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "sub = pd.read_csv('../data/submission.csv')\n",
    "\n",
    "# 特征工程\n",
    "df_source = train.merge(label,on='ID',how='left')\n",
    "test['label'] = -1\n",
    "train = train.merge(label,on='ID',how='left')\n",
    "data = pd.concat([train, test])\n",
    "\n",
    "# 提取时间特征\n",
    "def get_time_fe(df):\n",
    "    df['day'] = df.date.apply(lambda x:int(x[8:10]))\n",
    "    df['hour'] = df.date.apply(lambda x:int(x[11:13]))\n",
    "    return df\n",
    "\n",
    "# 时间分箱\n",
    "def getSeg(x):\n",
    "    if x >=0 and x<= 3:\n",
    "        return 1\n",
    "    elif x>=4 and x<=12:\n",
    "        return 2\n",
    "    elif x>=13 and x<=18:\n",
    "        return 3\n",
    "    elif x>=19 and x<=23:\n",
    "        return 1\n",
    "\n",
    "# count统计特征\n",
    "cross_feature = []\n",
    "def get_cross_fe(df):\n",
    "    first_feature = [ 'B2', 'B3']\n",
    "    second_feature = ['C1','C2','C3','D1','A1','A2','A3']\n",
    "    for feat_1 in first_feature:\n",
    "        for feat_2 in second_feature:\n",
    "            col_name = \"cross_\" + feat_1 + \"_and_\" + feat_2\n",
    "            cross_feature.append(col_name)\n",
    "            df[col_name] = df[feat_1].astype(str).values + '_' + df[feat_2].astype(str).values\n",
    "    return df\n",
    "\n",
    "# 获取nunique特征\n",
    "def get_nunique_1_fe(df):\n",
    "    adid_nuq = [ 'hour','E1','E14','B2','B3']\n",
    "    for feat in adid_nuq:\n",
    "        gp1 = df.groupby('A2')[feat].nunique().reset_index().rename(columns={feat: \"A2_%s_nuq_num\" % feat})\n",
    "        gp2 = df.groupby(feat)['A2'].nunique().reset_index().rename(columns={'A2': \"%s_A2_nuq_num\" % feat})\n",
    "        df = pd.merge(df, gp1, how='left', on=['A2'])\n",
    "        df = pd.merge(df, gp2, how='left', on=[feat])\n",
    "    return df\n",
    "def get_nunique_2_fe(df):\n",
    "    adid_nuq = [ 'E1','E14']\n",
    "    for feat in adid_nuq:\n",
    "        gp1 = df.groupby('hour')[feat].nunique().reset_index().rename(columns={feat: \"hour_%s_nuq_num\" % feat})\n",
    "        gp2 = df.groupby(feat)['hour'].nunique().reset_index().rename(columns={'hour': \"%s_hour_nuq_num\" % feat})\n",
    "        df = pd.merge(df, gp1, how='left', on=['hour'])\n",
    "        df = pd.merge(df, gp2, how='left', on=[feat])\n",
    "    return df\n",
    "\n",
    "def get_nunique_4_fe(df):\n",
    "    adid_nuq = [ 'B2','B3']\n",
    "    for feat in adid_nuq:\n",
    "        gp1 = df.groupby('A1')[feat].nunique().reset_index().rename(columns={feat: \"A1_%s_nuq_num\" % feat})\n",
    "        gp2 = df.groupby(feat)['A1'].nunique().reset_index().rename(columns={'A1': \"%s_A1_nuq_num\" % feat})\n",
    "        df = pd.merge(df, gp1, how='left', on=['A1'])\n",
    "        df = pd.merge(df, gp2, how='left', on=[feat])\n",
    "    return df\n",
    "\n",
    "# 应用特征工程\n",
    "data = get_time_fe(data)\n",
    "data = get_cross_fe(data)\n",
    "data = get_nunique_1_fe(data)\n",
    "data = get_nunique_2_fe(data)\n",
    "\n",
    "# 标签编码\n",
    "cate_feature = ['A1','A2','A3','B1','B2','B3','C1','C2','C3','E2','E3','E5','E7','E9','E10','E13','E16','E17','E19','E21','E22']\n",
    "cate_features = cate_feature + cross_feature\n",
    "for item in cate_features:\n",
    "    data[item] = LabelEncoder().fit_transform(data[item])\n",
    "\n",
    "# 计数特征\n",
    "def feature_count(data, features=[]):\n",
    "    new_feature = 'count'\n",
    "    for i in features:\n",
    "        new_feature += '_' + i\n",
    "    try:\n",
    "        del data[new_feature]\n",
    "    except:\n",
    "        pass\n",
    "    temp = data.groupby(features).size().reset_index().rename(columns={0: new_feature})\n",
    "    data = data.merge(temp, 'left', on=features)\n",
    "    return data\n",
    "\n",
    "for i in cross_feature:\n",
    "    n = data[i].nunique()\n",
    "    if n > 5:\n",
    "        data = feature_count(data, [i])\n",
    "    else:\n",
    "        print(i, ':', n)\n",
    "\n",
    "# 比例特征\n",
    "label_feature =[ 'A2', 'A3','hour']\n",
    "data_temp = data[label_feature]\n",
    "df_feature = pd.DataFrame()\n",
    "data_temp['cnt'] = 1\n",
    "print('Begin ratio clcik...')\n",
    "col_type = label_feature.copy()\n",
    "n = len(col_type)\n",
    "for i in range(n):\n",
    "    col_name = \"ratio_click_of_\" + col_type[i]\n",
    "    df_feature[col_name] = (\n",
    "                    data_temp[col_type[i]].map(data_temp[col_type[i]].value_counts()) / len(data) * 100).astype(int)            \n",
    "data = pd.concat([data, df_feature], axis=1)\n",
    "print('The end')\n",
    "\n",
    "# 数据拆分\n",
    "train_df = data[data['label'] != -1]\n",
    "test_df = data[data['label'] == -1]\n",
    "\n",
    "# 特征筛选\n",
    "del_feature = ['ID','day','date','label','D2'] + cross_feature\n",
    "features = [i for i in train_df.columns if i not in del_feature]\n",
    "\n",
    "train_x = train_df[features]\n",
    "train_y = train_df['label'].values\n",
    "test_x = test_df[features]\n",
    "\n",
    "# 设置随机种子\n",
    "np.random.seed(42)\n",
    "\n",
    "# 获取特征信息\n",
    "if isinstance(train_x, pd.DataFrame):\n",
    "    n_features = train_x.shape[1]\n",
    "    feature_names = train_x.columns.tolist()\n",
    "else:\n",
    "    n_features = train_x.shape[1]\n",
    "    feature_names = [f'特征{i+1}' for i in range(n_features)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基模型参数配置\n",
    "def get_base_model_params():\n",
    "    # LightGBM参数\n",
    "    lgb_params1 = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "        'max_depth': -1,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 0,\n",
    "        'seed': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    lgb_params2 = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'learning_rate': 0.03,\n",
    "        'num_leaves': 63,\n",
    "        'max_depth': 8,\n",
    "        'feature_fraction': 0.7,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'bagging_freq': 3,\n",
    "        'verbose': 0,\n",
    "        'seed': 43,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    # XGBoost参数\n",
    "    xgb_params1 = {\n",
    "        'booster': 'gbtree',\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'learning_rate': 0.05,\n",
    "        'max_depth': 5,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'scale_pos_weight': 1,\n",
    "        'seed': 44,\n",
    "        'n_jobs': -1,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    xgb_params2 = {\n",
    "        'booster': 'gbtree',\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'auc',\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': 7,\n",
    "        'subsample': 0.7,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'gamma': 0.1,\n",
    "        'seed': 45,\n",
    "        'n_jobs': -1,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    return [\n",
    "        {'type': 'lgb', 'params': lgb_params1},\n",
    "        {'type': 'lgb', 'params': lgb_params2},\n",
    "        {'type': 'xgb', 'params': xgb_params1},\n",
    "        {'type': 'xgb', 'params': xgb_params2}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改进的stacking模型（元模型改为逻辑回归）\n",
    "def stacking_model(X, y):\n",
    "    model_params = get_base_model_params()\n",
    "    print(f\"将使用{len(model_params)}个基模型（2个LightGBM + 2个XGBoost）\")\n",
    "    \n",
    "    models = []\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # 训练基模型\n",
    "    print(\"开始训练基模型...\")\n",
    "    for i, config in enumerate(model_params, 1):\n",
    "        model_type = config['type']\n",
    "        params = config['params']\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                X_train = X.iloc[train_idx]\n",
    "                X_val = X.iloc[val_idx]\n",
    "            else:\n",
    "                X_train = X[train_idx]\n",
    "                X_val = X[val_idx]\n",
    "            \n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            \n",
    "            if model_type == 'lgb':\n",
    "                lgb_train = lgb.Dataset(X_train, y_train)\n",
    "                lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "                model = lgb.train(\n",
    "                    params,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=1000,\n",
    "                    valid_sets=[lgb_train, lgb_val],\n",
    "                    valid_names=['train', 'valid'],\n",
    "                    early_stopping_rounds=50,\n",
    "                    verbose_eval=0\n",
    "                )\n",
    "                y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "            \n",
    "            elif model_type == 'xgb':\n",
    "                xgb_train = xgb.DMatrix(X_train, label=y_train)\n",
    "                xgb_val = xgb.DMatrix(X_val, label=y_val)\n",
    "                model = xgb.train(\n",
    "                    params,\n",
    "                    xgb_train,\n",
    "                    num_boost_round=1000,\n",
    "                    evals=[(xgb_train, 'train'), (xgb_val, 'valid')],\n",
    "                    early_stopping_rounds=50,\n",
    "                    verbose_eval=0\n",
    "                )\n",
    "                y_pred = model.predict(xgb_val, ntree_limit=model.best_iteration)\n",
    "            \n",
    "            if fold == kf.n_splits - 1:\n",
    "                models.append({\n",
    "                    'model': model,\n",
    "                    'type': model_type\n",
    "                })\n",
    "                \n",
    "                auc = roc_auc_score(y_val, y_pred)\n",
    "                print(f\"基模型 {i}（{model_type}）验证集AUC: {auc:.4f}\")\n",
    "                break\n",
    "    \n",
    "    # 生成元特征\n",
    "    print(\"\\n生成元特征...\")\n",
    "    meta_features = np.zeros((X.shape[0], len(models)))\n",
    "    for i, model_info in enumerate(models):\n",
    "        model = model_info['model']\n",
    "        model_type = model_info['type']\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            meta_features[:, i] = model.predict(X, num_iteration=model.best_iteration)\n",
    "        elif model_type == 'xgb':\n",
    "            dmatrix = xgb.DMatrix(X)\n",
    "            meta_features[:, i] = model.predict(dmatrix, ntree_limit=model.best_iteration)\n",
    "    \n",
    "    # 元模型改为逻辑回归（增加标准化处理）\n",
    "    print(\"\\n开始训练逻辑回归元模型...\")\n",
    "    scaler = StandardScaler()  # 逻辑回归需要特征标准化\n",
    "    meta_features_scaled = scaler.fit_transform(meta_features)  # 标准化元特征\n",
    "    \n",
    "    # 逻辑回归参数\n",
    "    meta_model = LogisticRegression(\n",
    "        C=1.0,  # 正则化强度，越小正则化越强\n",
    "        penalty='l2',  # L2正则化\n",
    "        solver='liblinear',  # 小数据集适合的求解器\n",
    "        class_weight='balanced',  # 处理类别不平衡\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    )\n",
    "    meta_model.fit(meta_features_scaled, y)\n",
    "    \n",
    "    print('元模型训练完毕！')\n",
    "    \n",
    "    return models, meta_model, scaler, model_params  # 返回标准化器用于后续预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "将使用4个基模型（2个LightGBM + 2个XGBoost）\n",
      "开始训练基模型...\n"
     ]
    }
   ],
   "source": [
    "# 训练stacking模型\n",
    "models, meta_model, scaler, model_params = stacking_model(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征重要性可视化（适配逻辑回归元模型）\n",
    "def plot_feature_importance(models, meta_model, feature_names, top_n=50, figsize=(12, 8)):\n",
    "    # 基模型特征重要性\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, model_info in enumerate(models):\n",
    "        model = model_info['model']\n",
    "        model_type = model_info['type']\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            importance = model.feature_importance(importance_type='gain')\n",
    "        elif model_type == 'xgb':\n",
    "            importance_dict = model.get_score(importance_type='gain')\n",
    "            importance = [importance_dict.get(f, 0) for f in feature_names]\n",
    "        \n",
    "        feat_imp = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importance,\n",
    "            'model': f'基模型 {i+1}（{model_type}）'\n",
    "        })\n",
    "        top_feat = feat_imp.sort_values('importance', ascending=False).head(top_n)\n",
    "        sns.barplot(x='importance', y='feature', data=top_feat, label=f'基模型 {i+1}（{model_type}）')\n",
    "    \n",
    "    plt.title(f'基模型特征重要性（前{top_n}）', fontsize=15)\n",
    "    plt.xlabel('重要性 (Gain)', fontsize=12)\n",
    "    plt.ylabel('特征', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 逻辑回归元模型特征重要性（系数）\n",
    "    plt.figure(figsize=figsize)\n",
    "    meta_coef = meta_model.coef_[0]  # 逻辑回归系数\n",
    "    meta_feat_imp = pd.DataFrame({\n",
    "        'feature': [f'基模型 {i+1}（{models[i][\"type\"]}）输出' for i in range(len(models))],\n",
    "        'importance': np.abs(meta_coef)  # 用系数绝对值表示重要性\n",
    "    })\n",
    "    meta_feat_imp = meta_feat_imp.sort_values('importance', ascending=False)\n",
    "    \n",
    "    sns.barplot(x='importance', y='feature', data=meta_feat_imp)\n",
    "    plt.title('逻辑回归元模型特征重要性（系数绝对值）', fontsize=15)\n",
    "    plt.xlabel('系数绝对值', fontsize=12)\n",
    "    plt.ylabel('元特征', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_feature_importance(models, meta_model, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多指标评估函数\n",
    "def calculate_metrics(y_true, y_pred_proba):\n",
    "    auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "    ks = max(tpr - fpr)\n",
    "    \n",
    "    thresholds = np.arange(0.1, 1.0, 0.1)\n",
    "    f1_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        f1_scores.append(f1_score(y_true, y_pred))\n",
    "        precision_scores.append(precision_score(y_true, y_pred))\n",
    "        recall_scores.append(recall_score(y_true, y_pred))\n",
    "    \n",
    "    best_f1_idx = np.argmax(f1_scores)\n",
    "    best_f1 = f1_scores[best_f1_idx]\n",
    "    best_threshold = thresholds[best_f1_idx]\n",
    "    \n",
    "    return {\n",
    "        'auc': auc,\n",
    "        'ks': ks,\n",
    "        'best_f1': best_f1,\n",
    "        'best_threshold': best_threshold,\n",
    "        'thresholds': thresholds,\n",
    "        'f1_scores': f1_scores,\n",
    "        'precision_scores': precision_scores,\n",
    "        'recall_scores': recall_scores\n",
    "    }\n",
    "\n",
    "# 评估指标可视化\n",
    "def plot_metrics(metrics, figsize=(15, 10)):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    fig.suptitle('模型评估指标', fontsize=16)\n",
    "    \n",
    "    # 关键指标\n",
    "    ax1 = axes[0, 0]\n",
    "    metrics_text = f\"AUC: {metrics['auc']:.4f}\\nKS: {metrics['ks']:.4f}\\n最佳F1: {metrics['best_f1']:.4f}\\n最佳阈值: {metrics['best_threshold']:.2f}\"\n",
    "    ax1.text(0.5, 0.5, metrics_text, fontsize=14, ha='center', va='center')\n",
    "    ax1.set_title('关键指标值')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # ROC曲线\n",
    "    ax2 = axes[0, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_val, y_pred_proba)\n",
    "    ax2.plot(fpr, tpr, label=f'ROC曲线 (AUC = {metrics[\"auc\"]:.4f})')\n",
    "    ax2.plot([0, 1], [0, 1], 'k--')\n",
    "    ax2.set_xlabel('假正例率 (FPR)')\n",
    "    ax2.set_ylabel('真正例率 (TPR)')\n",
    "    ax2.set_title('ROC曲线')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # KS曲线\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.plot(fpr, label='FPR')\n",
    "    ax3.plot(tpr, label='TPR')\n",
    "    ax3.plot(tpr - fpr, label=f'KS ({metrics[\"ks\"]:.4f})')\n",
    "    ax3.set_xlabel('阈值')\n",
    "    ax3.set_ylabel('比例')\n",
    "    ax3.set_title('KS曲线')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # 阈值相关指标\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.plot(metrics['thresholds'], metrics['f1_scores'], label='F1分数')\n",
    "    ax4.plot(metrics['thresholds'], metrics['precision_scores'], label='精确率')\n",
    "    ax4.plot(metrics['thresholds'], metrics['recall_scores'], label='召回率')\n",
    "    ax4.axvline(x=metrics['best_threshold'], color='r', linestyle='--', label=f'最佳阈值: {metrics[\"best_threshold\"]:.2f}')\n",
    "    ax4.set_xlabel('阈值')\n",
    "    ax4.set_ylabel('分数')\n",
    "    ax4.set_title('不同阈值下的分类指标')\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# 模型评估与可视化\n",
    "print(\"\\n交叉验证评估结果与可视化...\")\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "final_auc_scores = []\n",
    "final_ks_scores = []\n",
    "final_f1_scores = []\n",
    "\n",
    "y_val = None\n",
    "y_pred_proba = None\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_x, train_y)):\n",
    "    val_meta_features = np.zeros((len(val_idx), len(models)))\n",
    "    \n",
    "    if isinstance(train_x, pd.DataFrame):\n",
    "        X_val = train_x.iloc[val_idx]\n",
    "    else:\n",
    "        X_val = train_x[val_idx]\n",
    "    \n",
    "    for i, model_info in enumerate(models):\n",
    "        model = model_info['model']\n",
    "        model_type = model_info['type']\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            val_meta_features[:, i] = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "        elif model_type == 'xgb':\n",
    "            xgb_val = xgb.DMatrix(X_val)\n",
    "            val_meta_features[:, i] = model.predict(xgb_val, ntree_limit=model.best_iteration)\n",
    "    \n",
    "    # 逻辑回归预测（需要标准化）\n",
    "    val_meta_scaled = scaler.transform(val_meta_features)\n",
    "    y_pred = meta_model.predict_proba(val_meta_scaled)[:, 1]  # 取正例概率\n",
    "    auc = roc_auc_score(train_y[val_idx], y_pred)\n",
    "    \n",
    "    metrics = calculate_metrics(train_y[val_idx], y_pred)\n",
    "    \n",
    "    final_auc_scores.append(auc)\n",
    "    final_ks_scores.append(metrics['ks'])\n",
    "    final_f1_scores.append(metrics['best_f1'])\n",
    "    \n",
    "    if fold == kf.n_splits - 1:\n",
    "        y_val = train_y[val_idx]\n",
    "        y_pred_proba = y_pred\n",
    "        fold_metrics = metrics\n",
    "\n",
    "print(f\"最终Stacking模型的平均AUC: {np.mean(final_auc_scores):.4f} ± {np.std(final_auc_scores):.4f}\")\n",
    "print(f\"最终Stacking模型的平均KS: {np.mean(final_ks_scores):.4f} ± {np.std(final_ks_scores):.4f}\")\n",
    "print(f\"最终Stacking模型的平均最佳F1: {np.mean(final_f1_scores):.4f} ± {np.std(final_f1_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(fold_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "print(\"\\n保存模型...\")\n",
    "stacking_model = {\n",
    "    'base_models': models,\n",
    "    'meta_model': meta_model,\n",
    "    'scaler': scaler,  # 保存标准化器\n",
    "    'model_params': model_params\n",
    "}\n",
    "\n",
    "with open('./models/stacking_model_logistic.pkl', 'wb') as f:\n",
    "    pickle.dump(stacking_model, f)\n",
    "\n",
    "print(\"模型已保存为 stacking_model_logistic.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测函数\n",
    "def predict_with_model(model_path, X):\n",
    "    with open(model_path, 'rb') as f:\n",
    "        stacking_model = pickle.load(f)\n",
    "    \n",
    "    base_models = stacking_model['base_models']\n",
    "    meta_model = stacking_model['meta_model']\n",
    "    scaler = stacking_model['scaler']  # 加载标准化器\n",
    "    \n",
    "    meta_features = np.zeros((X.shape[0], len(base_models)))\n",
    "    for i, model_info in enumerate(base_models):\n",
    "        model = model_info['model']\n",
    "        model_type = model_info['type']\n",
    "        \n",
    "        if model_type == 'lgb':\n",
    "            meta_features[:, i] = model.predict(X, num_iteration=model.best_iteration)\n",
    "        elif model_type == 'xgb':\n",
    "            dmatrix = xgb.DMatrix(X)\n",
    "            meta_features[:, i] = model.predict(dmatrix, ntree_limit=model.best_iteration)\n",
    "    \n",
    "    # 逻辑回归预测（标准化元特征）\n",
    "    meta_features_scaled = scaler.transform(meta_features)\n",
    "    predictions = meta_model.predict_proba(meta_features_scaled)[:, 1]  # 取正例概率\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 执行预测\n",
    "predictions = predict_with_model('./models/stacking_model_logistic.pkl', test_x)\n",
    "\n",
    "sub['label'] = predictions\n",
    "sub.to_csv('./sub/stacking_model_logistic.csv',index = False)\n",
    "\n",
    "print(\"预测结果:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
